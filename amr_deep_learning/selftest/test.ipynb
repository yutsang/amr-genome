{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing .fna files: 100%|██████████| 36/36 [00:00<00:00, 67.05it/s]\n",
      "Extracting k-mers: 100%|██████████| 128718/128718 [02:32<00:00, 845.09it/s] \n",
      "Mapping labels: 100%|██████████| 128718/128718 [00:33<00:00, 3891.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ytsang/anaconda3/envs/test_tf/lib/python3.12/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3218/3218\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 3ms/step - accuracy: 0.5928 - loss: 0.8397 - val_accuracy: 0.6893 - val_loss: 0.6075\n",
      "Epoch 2/10\n",
      "\u001b[1m3218/3218\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3ms/step - accuracy: 0.6659 - loss: 0.6192 - val_accuracy: 0.6877 - val_loss: 0.5895\n",
      "Epoch 3/10\n",
      "\u001b[1m3218/3218\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3ms/step - accuracy: 0.6820 - loss: 0.5993 - val_accuracy: 0.6906 - val_loss: 0.5834\n",
      "Epoch 4/10\n",
      "\u001b[1m3218/3218\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3ms/step - accuracy: 0.6868 - loss: 0.5911 - val_accuracy: 0.7018 - val_loss: 0.5690\n",
      "Epoch 5/10\n",
      "\u001b[1m3218/3218\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3ms/step - accuracy: 0.6911 - loss: 0.5872 - val_accuracy: 0.6889 - val_loss: 0.5747\n",
      "Epoch 6/10\n",
      "\u001b[1m3218/3218\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3ms/step - accuracy: 0.6928 - loss: 0.5815 - val_accuracy: 0.6990 - val_loss: 0.5702\n",
      "Epoch 7/10\n",
      "\u001b[1m3218/3218\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3ms/step - accuracy: 0.6926 - loss: 0.5818 - val_accuracy: 0.7034 - val_loss: 0.5666\n",
      "Epoch 8/10\n",
      "\u001b[1m3218/3218\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3ms/step - accuracy: 0.6972 - loss: 0.5759 - val_accuracy: 0.7005 - val_loss: 0.5660\n",
      "Epoch 9/10\n",
      "\u001b[1m3218/3218\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3ms/step - accuracy: 0.6975 - loss: 0.5774 - val_accuracy: 0.7050 - val_loss: 0.5669\n",
      "Epoch 10/10\n",
      "\u001b[1m3218/3218\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3ms/step - accuracy: 0.7001 - loss: 0.5734 - val_accuracy: 0.7035 - val_loss: 0.5658\n",
      "\u001b[1m805/805\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7067 - loss: 0.5627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.7035\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "from Bio import SeqIO\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, Dropout\n",
    "from tqdm import tqdm\n",
    "from pathos.multiprocessing import ProcessingPool as Pool\n",
    "\n",
    "# Configure TensorFlow to use GPU\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "try:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "except:\n",
    "    # Invalid device or cannot modify virtual devices once initialized.\n",
    "    pass\n",
    "\n",
    "# Function to parse a single .fna file\n",
    "def parse_fna_file(filepath):\n",
    "    sequences = []\n",
    "    filenames = []\n",
    "    for record in SeqIO.parse(filepath, \"fasta\"):\n",
    "        sequences.append(str(record.seq))\n",
    "        filenames.append(os.path.basename(filepath))\n",
    "    return sequences, filenames\n",
    "\n",
    "# Function to parse all .fna files using multiprocessing\n",
    "def parse_fna_files(directory):\n",
    "    filepaths = [os.path.join(directory, filename) for filename in os.listdir(directory) if filename.endswith(\".fna\")]\n",
    "    with Pool() as pool:\n",
    "        results = list(tqdm(pool.imap(parse_fna_file, filepaths), total=len(filepaths), desc=\"Parsing .fna files\"))\n",
    "    sequences = [seq for result in results for seq in result[0]]\n",
    "    filenames = [fname for result in results for fname in result[1]]\n",
    "    return sequences, filenames\n",
    "\n",
    "# Function to get k-mers from a sequence\n",
    "def get_kmers(sequence, size=6):\n",
    "    return [sequence[x:x+size].lower() for x in range(len(sequence) - size + 1)]\n",
    "\n",
    "# Function to extract features from sequences using multiprocessing\n",
    "def extract_features(sequences, k=6):\n",
    "    with Pool() as pool:\n",
    "        kmers = list(tqdm(pool.imap(lambda seq: ' '.join(get_kmers(seq, k)), sequences), total=len(sequences), desc=\"Extracting k-mers\"))\n",
    "    vectorizer = CountVectorizer()\n",
    "    X = vectorizer.fit_transform(kmers)\n",
    "    return X, vectorizer\n",
    "\n",
    "def main():\n",
    "    # Directory containing your .fna files\n",
    "    fna_directory = './fna/'\n",
    "\n",
    "    # Parse .fna files\n",
    "    sequences, filenames = parse_fna_files(fna_directory)\n",
    "\n",
    "    # Extract features\n",
    "    X, vectorizer = extract_features(sequences)\n",
    "\n",
    "    # Load labels from a CSV file\n",
    "    labels_df = pd.read_csv('label.csv')\n",
    "    labels_df['resistant_phenotype'] = labels_df['resistant_phenotype'].map({'resistant': 1, 'susceptible': 0})\n",
    "\n",
    "    # Map labels to sequences based on filenames\n",
    "    labels = []\n",
    "    for filename in tqdm(filenames, desc=\"Mapping labels\"):\n",
    "        genbank_id = '.'.join(filename.split('.')[:2])  # Correctly handle filenames with multiple periods\n",
    "        label_row = labels_df.loc[labels_df['genbank_id'] == genbank_id, 'resistant_phenotype']\n",
    "        if not label_row.empty:\n",
    "            label = label_row.values[0]\n",
    "            labels.append(label)\n",
    "        else:\n",
    "            print(f\"Warning: GenBank ID {genbank_id} not found in labels file. Skipping this file.\")\n",
    "            # Optionally, you can append a default label or handle it differently\n",
    "            # labels.append(default_label)\n",
    "\n",
    "    # Convert labels to a numpy array and ensure they are numerical\n",
    "    y = pd.Series(labels).astype(float).values\n",
    "\n",
    "    # Split data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Perform PCA for dimension reduction\n",
    "    pca = PCA(n_components=100)  # Adjust the number of components as needed\n",
    "    X_train_pca = pca.fit_transform(X_train.toarray())\n",
    "    X_test_pca = pca.transform(X_test.toarray())\n",
    "\n",
    "    # Define the deep learning model\n",
    "    model = Sequential([\n",
    "        Dense(128, activation='relu', input_shape=(X_train_pca.shape[1],)),\n",
    "        Dropout(0.5),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Train the model with a progress bar\n",
    "    history = model.fit(X_train_pca, y_train, epochs=10, batch_size=32, validation_data=(X_test_pca, y_test), verbose=1)\n",
    "\n",
    "    # Evaluate the model\n",
    "    loss, accuracy = model.evaluate(X_test_pca, y_test)\n",
    "    print(f'Test Accuracy: {accuracy:.4f}')\n",
    "\n",
    "    # Save the model\n",
    "    model.save('amr_prediction_model.h5')\n",
    "\n",
    "    # Load the model for future use\n",
    "    # model = load_model('amr_prediction_model.h5')\n",
    "\n",
    "    # Make predictions\n",
    "    # predictions = model.predict(X_test_pca)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ytsang/anaconda3/envs/test_tf/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.compile(loss=tf.keras.losses.categorical_crossentropy,\n",
    "              optimizer=tf.keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 28ms/step - accuracy: 0.1037 - loss: 2.3006 - val_accuracy: 0.1732 - val_loss: 2.2710\n",
      "Epoch 2/10\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 28ms/step - accuracy: 0.1725 - loss: 2.2666 - val_accuracy: 0.3905 - val_loss: 2.2273\n",
      "Epoch 3/10\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 28ms/step - accuracy: 0.2656 - loss: 2.2240 - val_accuracy: 0.5631 - val_loss: 2.1692\n",
      "Epoch 4/10\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 28ms/step - accuracy: 0.3574 - loss: 2.1669 - val_accuracy: 0.6418 - val_loss: 2.0901\n",
      "Epoch 5/10\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 29ms/step - accuracy: 0.4417 - loss: 2.0882 - val_accuracy: 0.6892 - val_loss: 1.9828\n",
      "Epoch 6/10\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 28ms/step - accuracy: 0.5007 - loss: 1.9840 - val_accuracy: 0.7179 - val_loss: 1.8410\n",
      "Epoch 7/10\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 27ms/step - accuracy: 0.5480 - loss: 1.8481 - val_accuracy: 0.7442 - val_loss: 1.6634\n",
      "Epoch 8/10\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 28ms/step - accuracy: 0.5891 - loss: 1.6829 - val_accuracy: 0.7685 - val_loss: 1.4626\n",
      "Epoch 9/10\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 28ms/step - accuracy: 0.6096 - loss: 1.5208 - val_accuracy: 0.7842 - val_loss: 1.2652\n",
      "Epoch 10/10\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 28ms/step - accuracy: 0.6335 - loss: 1.3598 - val_accuracy: 0.8021 - val_loss: 1.0944\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7f8b58675bb0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train,\n",
    "          batch_size=128,\n",
    "          epochs=10,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 1.0944098234176636\n",
      "Test accuracy: 0.8021000027656555\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
